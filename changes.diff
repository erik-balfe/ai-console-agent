diff --git a/.gitignore b/.gitignore
index 09af931..f786792 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,3 +3,4 @@ node_modules
 .aider.input.history
 .aider.tags.cache.v3
 .aider.chat.history.md
+.aider*
diff --git a/src/index.ts b/src/index.ts
index 35fa940..ced7f43 100644
--- a/src/index.ts
+++ b/src/index.ts
@@ -1,45 +1,9 @@
 import { config } from "dotenv";
-import { OpenAI, SimpleDirectoryReader, VectorStoreIndex } from "llamaindex";
-import { stdin } from "process";
+import { runAgent } from "./ai/agent";
+import { getPipedInput, parseArguments } from "./cli/interface";
 
-// Load environment variables
 config();
 
-// Initialize LLM client
-const llm = new OpenAI({ apiKey: process.env.LLM_API_KEY });
-
-// Function to make a simple query to the LLM
-async function queryLLM(input: string): Promise<string> {
-  const serviceContext = ServiceContext.fromDefaults({ llm });
-  const documents = await new SimpleDirectoryReader().loadData({ text: input });
-  const index = await VectorStoreIndex.fromDocuments(documents, { serviceContext });
-  const queryEngine = index.asQueryEngine();
-  const response = await queryEngine.query(input);
-  return response.toString();
-}
-
-function parseArguments(args: string[]): { command: string; flags: string[] } {
-  const [, , ...cliArgs] = args;
-  const command = cliArgs[0] || "";
-  const flags = cliArgs.slice(1);
-  return { command, flags };
-}
-
-async function getPipedInput(): Promise<string> {
-  return new Promise((resolve) => {
-    let data = "";
-    stdin.on("readable", () => {
-      let chunk;
-      while (null !== (chunk = stdin.read())) {
-        data += chunk;
-      }
-    });
-    stdin.on("end", () => {
-      resolve(data.trim());
-    });
-  });
-}
-
 async function main() {
   const { command, flags } = parseArguments(process.argv);
   console.log(`Command: ${command}`);
@@ -49,15 +13,19 @@ async function main() {
   if (pipedInput) {
     console.log(`Piped input: ${pipedInput}`);
 
-    // Use the piped input to query the LLM
+    if (pipedInput.length > 1000) {
+      console.error("Input is too long. Please limit your input to 1000 characters.");
+      return;
+    }
+
     try {
-      const llmResponse = await queryLLM(pipedInput);
-      console.log("LLM Response:", llmResponse);
+      const agentResponse = await runAgent(pipedInput);
+      console.log("Agent Response:", agentResponse);
     } catch (error) {
-      console.error("Error querying LLM:", error);
+      console.error("Error running agent:", error);
     }
   } else {
-    console.log("No piped input received. Please provide input to query the LLM.");
+    console.log("No piped input received. Please provide input for the agent.");
   }
 }
 
